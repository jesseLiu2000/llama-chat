{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to GPT4 Authors: we introduce --, a platform conclude 20 scenarios performance in ChatGPT compared with former language model.(repo) Overview Large language models, like ChatGPT, have revolutionized the field of natural language processing by achieving state-of-the-art performance on a wide range of tasks such as language translation, question-answering, and text generation. ChatGPT is among the most prominent LLMs that is based on the GPT-3.5 architecture and has been trained on a massive amount of textual data. Despite its impressive achievements, there is a need for a comprehensive evaluation of ChatGPT's performance on a variety of natural language tasks. Such an evaluation would help researchers and practitioners understand the strengths and limitations of ChatGPT, and provide insights into ways to improve it. Therefore, we aim to provide the first holistic evaluation of ChatGPT on various natural language tasks. This evaluation will cover a broad range of tasks, including language modeling, sentiment analysis, text classification, named entity recognition, and more. By evaluating ChatGPT on these tasks, we will gain a better understanding of its capabilities and limitations, and provide a benchmark for future research in the field of natural language processing. Result Scenario/Model MMLU - EM BoolQ - EM NarrativeQA - F1 NaturalQuestions (closed-book) - F1 NaturalQuestions (open-book) - F1 QuAC - F1 HellaSwag - EM OpenbookQA - EM TruthfulQA - EM MS MARCO (regular) - RR@10 MS MARCO (TREC) - NDCG@10 CNN/DailyMail - ROUGE-2 XSUM - ROUGE-2 IMDB - EM CivilComments - EM RAFT - EM \u2191 [ sort ] Chatgpt 5-shot (Ours) 0.595 0.811 0.691 0.389 0.5115 0.841 0.832 0.627 0.171 Cohere Command beta (52.4B) 0.452 0.856 0.752 0.372 0.76 0.432 0.811 0.582 0.269 0.472 0.762 0.161 0.152 0.96 0.601 0.667 text-davinci-002 0.568 0.877 0.727 0.383 0.713 0.445 0.815 0.594 0.61 0.421 0.664 0.153 0.144 0.948 0.668 0.733 text-davinci-003 0.569 0.881 0.727 0.406 0.77 0.525 0.822 0.646 0.593 0.368 0.644 0.156 0.124 0.848 0.684 0.759 TNLG v2 (530B) 0.469 0.809 0.722 0.384 0.642 0.39 0.799 0.562 0.251 0.377 0.643 0.161 0.169 0.941 0.601 0.679 Anthropic-LM v4-s3 (52B) 0.481 0.815 0.728 0.288 0.686 0.431 0.807 0.558 0.368 - - 0.154 0.134 0.934 0.61 0.699 J1-Grande v2 beta (17B) 0.445 0.812 0.725 0.337 0.625 0.392 0.764 0.56 0.306 0.285 0.46 0.146 0.152 0.957 0.546 0.679 Luminous Supreme (70B) 0.38 0.775 0.711 0.293 0.649 0.37 - - 0.222 - - 0.15 0.136 0.959 0.562 0.653 Cohere Command beta (6.1B) 0.406 0.798 0.709 0.229 0.717 0.375 0.752 0.55 0.203 0.434 0.709 0.153 0.122 0.961 0.54 0.634 Cohere xlarge v20221108 (52.4B) 0.382 0.762 0.672 0.361 0.628 0.374 0.81 0.588 0.169 0.315 0.55 0.153 0.153 0.956 0.524 0.624 OPT (175B) 0.318 0.793 0.671 0.297 0.615 0.36 0.791 0.586 0.25 0.288 0.448 0.146 0.155 0.947 0.505 0.606 Cohere xlarge v20220609 (52.4B) 0.353 0.718 0.65 0.312 0.595 0.361 0.811 0.55 0.198 0.273 0.459 0.144 0.129 0.956 0.532 0.633 davinci (175B) 0.422 0.722 0.687 0.329 0.625 0.36 0.775 0.586 0.194 0.211 0.378 0.127 0.126 0.933 0.532 0.642 GLM (130B) 0.344 0.784 0.706 0.148 0.642 0.272 - - 0.218 - - 0.154 0.132 0.955 0.5 0.598 J1-Jumbo v1 (178B) 0.259 0.776 0.695 0.293 0.595 0.358 0.765 0.534 0.175 0.21 0.363 0.144 0.129 0.943 0.553 0.681 Luminous Extended (30B) 0.321 0.767 0.665 0.254 0.609 0.349 - - 0.221 - - 0.139 0.124 0.947 0.524 0.523 BLOOM (176B) 0.299 0.704 0.662 0.216 0.621 0.361 0.744 0.534 0.205 0.236 0.386 0.08 0.03 0.945 0.62 0.592 OPT (66B) 0.276 0.76 0.638 0.258 0.596 0.357 0.745 0.534 0.201 0.237 0.482 0.136 0.126 0.917 0.506 0.557 J1-Grande v1 (17B) 0.27 0.722 0.672 0.233 0.578 0.362 0.739 0.52 0.193 0.161 0.341 0.143 0.122 0.953 0.529 0.658 Cohere large v20220720 (13.1B) 0.324 0.725 0.625 0.232 0.573 0.338 0.736 0.542 0.181 0.19 0.33 0.126 0.108 0.933 0.507 0.596 text-curie-001 0.237 0.62 0.582 0.175 0.571 0.358 0.676 0.514 0.257 0.271 0.507 0.152 0.076 0.923 0.537 0.489 GPT-NeoX (20B) 0.276 0.683 0.599 0.193 0.596 0.326 0.718 0.524 0.216 0.184 0.398 0.123 0.102 0.948 0.516 0.505 Luminous Base (13B) 0.27 0.719 0.605 0.202 0.568 0.334 - - 0.182 - - 0.11 0.105 0.939 0.544 0.473 Cohere medium v20221108 (6.1B) 0.254 0.7 0.61 0.199 0.517 0.314 0.726 0.538 0.215 0.175 0.373 0.121 0.099 0.935 0.5 0.591 TNLG v2 (6.7B) 0.242 0.698 0.631 0.21 0.561 0.345 0.704 0.478 0.167 0.158 0.332 0.146 0.11 0.927 0.532 0.525 J1-Large v1 (7.5B) 0.241 0.683 0.623 0.19 0.532 0.328 0.7 0.514 0.197 0.147 0.292 0.134 0.102 0.956 0.532 0.545 GPT-J (6B) 0.249 0.649 0.545 0.156 0.559 0.33 0.663 0.514 0.199 0.152 0.345 0.131 0.096 0.939 0.52 0.619 curie (6.7B) 0.243 0.656 0.604 0.199 0.552 0.321 0.682 0.502 0.232 0.162 0.3 0.113 0.091 0.889 0.539 0.49 Cohere medium v20220720 (6.1B) 0.279 0.659 0.559 0.177 0.504 0.279 0.706 0.496 0.19 0.152 0.374 0.077 0.087 0.935 0.504 0.52 text-babbage-001 0.229 0.451 0.429 0.07 0.33 0.284 0.561 0.452 0.233 0.208 0.449 0.151 0.046 0.913 0.499 0.509 T0pp (11B) 0.407 0 0.151 0.039 0.19 0.121 - - 0.377 - - 0.122 0.09 0.207 0.234 0.118 UL2 (20B) 0.291 0.746 0.083 0.204 0.349 0.144 - - 0.193 - - 0.03 0.058 0.337 0.521 0.404 T5 (11B) 0.29 0.761 0.086 0.194 0.477 0.116 - - 0.133 - - 0.043 0.015 0.379 0.509 0.37 Cohere small v20220720 (410M) 0.264 0.457 0.294 0.078 0.309 0.219 0.483 0.348 0.217 - 0.304 0.063 0.033 0.578 0.501 0.492 ada (350M) 0.243 0.581 0.326 0.082 0.365 0.242 0.435 0.38 0.215 0.102 0.29 0.09 0.022 0.849 0.517 0.423 babbage (1.3B) 0.235 0.574 0.491 0.119 0.451 0.273 0.555 0.438 0.188 0.122 0.317 0.079 0.045 0.597 0.519 0.455 text-ada-001 0.238 0.464 0.238 0.025 0.149 0.176 0.429 0.346 0.232 0.134 0.302 0.136 0.034 0.822 0.503 0.406 YaLM (100B) 0.243 0.634 0.252 0.068 0.227 0.162 - - 0.202 - - 0.017 Future Work Develop methods to mitigate the potential safety risks associated with using large-scale language models like ChatGPT, such as automatic red teaming, auditing, and adaptive testing. Publish the benchmark dataset and code on a public repository such as GitHub to facilitate future research in the field. Write a research paper detailing the methodology, results, and insights gained from the benchmark study and share it with the scientific community to encourage further research and collaboration. Acknowledgments","title":"home"},{"location":"#welcome-to-gpt4","text":"Authors: we introduce --, a platform conclude 20 scenarios performance in ChatGPT compared with former language model.(repo)","title":"Welcome to GPT4"},{"location":"#overview","text":"Large language models, like ChatGPT, have revolutionized the field of natural language processing by achieving state-of-the-art performance on a wide range of tasks such as language translation, question-answering, and text generation. ChatGPT is among the most prominent LLMs that is based on the GPT-3.5 architecture and has been trained on a massive amount of textual data. Despite its impressive achievements, there is a need for a comprehensive evaluation of ChatGPT's performance on a variety of natural language tasks. Such an evaluation would help researchers and practitioners understand the strengths and limitations of ChatGPT, and provide insights into ways to improve it. Therefore, we aim to provide the first holistic evaluation of ChatGPT on various natural language tasks. This evaluation will cover a broad range of tasks, including language modeling, sentiment analysis, text classification, named entity recognition, and more. By evaluating ChatGPT on these tasks, we will gain a better understanding of its capabilities and limitations, and provide a benchmark for future research in the field of natural language processing.","title":"Overview"},{"location":"#result","text":"Scenario/Model MMLU - EM BoolQ - EM NarrativeQA - F1 NaturalQuestions (closed-book) - F1 NaturalQuestions (open-book) - F1 QuAC - F1 HellaSwag - EM OpenbookQA - EM TruthfulQA - EM MS MARCO (regular) - RR@10 MS MARCO (TREC) - NDCG@10 CNN/DailyMail - ROUGE-2 XSUM - ROUGE-2 IMDB - EM CivilComments - EM RAFT - EM \u2191 [ sort ] Chatgpt 5-shot (Ours) 0.595 0.811 0.691 0.389 0.5115 0.841 0.832 0.627 0.171 Cohere Command beta (52.4B) 0.452 0.856 0.752 0.372 0.76 0.432 0.811 0.582 0.269 0.472 0.762 0.161 0.152 0.96 0.601 0.667 text-davinci-002 0.568 0.877 0.727 0.383 0.713 0.445 0.815 0.594 0.61 0.421 0.664 0.153 0.144 0.948 0.668 0.733 text-davinci-003 0.569 0.881 0.727 0.406 0.77 0.525 0.822 0.646 0.593 0.368 0.644 0.156 0.124 0.848 0.684 0.759 TNLG v2 (530B) 0.469 0.809 0.722 0.384 0.642 0.39 0.799 0.562 0.251 0.377 0.643 0.161 0.169 0.941 0.601 0.679 Anthropic-LM v4-s3 (52B) 0.481 0.815 0.728 0.288 0.686 0.431 0.807 0.558 0.368 - - 0.154 0.134 0.934 0.61 0.699 J1-Grande v2 beta (17B) 0.445 0.812 0.725 0.337 0.625 0.392 0.764 0.56 0.306 0.285 0.46 0.146 0.152 0.957 0.546 0.679 Luminous Supreme (70B) 0.38 0.775 0.711 0.293 0.649 0.37 - - 0.222 - - 0.15 0.136 0.959 0.562 0.653 Cohere Command beta (6.1B) 0.406 0.798 0.709 0.229 0.717 0.375 0.752 0.55 0.203 0.434 0.709 0.153 0.122 0.961 0.54 0.634 Cohere xlarge v20221108 (52.4B) 0.382 0.762 0.672 0.361 0.628 0.374 0.81 0.588 0.169 0.315 0.55 0.153 0.153 0.956 0.524 0.624 OPT (175B) 0.318 0.793 0.671 0.297 0.615 0.36 0.791 0.586 0.25 0.288 0.448 0.146 0.155 0.947 0.505 0.606 Cohere xlarge v20220609 (52.4B) 0.353 0.718 0.65 0.312 0.595 0.361 0.811 0.55 0.198 0.273 0.459 0.144 0.129 0.956 0.532 0.633 davinci (175B) 0.422 0.722 0.687 0.329 0.625 0.36 0.775 0.586 0.194 0.211 0.378 0.127 0.126 0.933 0.532 0.642 GLM (130B) 0.344 0.784 0.706 0.148 0.642 0.272 - - 0.218 - - 0.154 0.132 0.955 0.5 0.598 J1-Jumbo v1 (178B) 0.259 0.776 0.695 0.293 0.595 0.358 0.765 0.534 0.175 0.21 0.363 0.144 0.129 0.943 0.553 0.681 Luminous Extended (30B) 0.321 0.767 0.665 0.254 0.609 0.349 - - 0.221 - - 0.139 0.124 0.947 0.524 0.523 BLOOM (176B) 0.299 0.704 0.662 0.216 0.621 0.361 0.744 0.534 0.205 0.236 0.386 0.08 0.03 0.945 0.62 0.592 OPT (66B) 0.276 0.76 0.638 0.258 0.596 0.357 0.745 0.534 0.201 0.237 0.482 0.136 0.126 0.917 0.506 0.557 J1-Grande v1 (17B) 0.27 0.722 0.672 0.233 0.578 0.362 0.739 0.52 0.193 0.161 0.341 0.143 0.122 0.953 0.529 0.658 Cohere large v20220720 (13.1B) 0.324 0.725 0.625 0.232 0.573 0.338 0.736 0.542 0.181 0.19 0.33 0.126 0.108 0.933 0.507 0.596 text-curie-001 0.237 0.62 0.582 0.175 0.571 0.358 0.676 0.514 0.257 0.271 0.507 0.152 0.076 0.923 0.537 0.489 GPT-NeoX (20B) 0.276 0.683 0.599 0.193 0.596 0.326 0.718 0.524 0.216 0.184 0.398 0.123 0.102 0.948 0.516 0.505 Luminous Base (13B) 0.27 0.719 0.605 0.202 0.568 0.334 - - 0.182 - - 0.11 0.105 0.939 0.544 0.473 Cohere medium v20221108 (6.1B) 0.254 0.7 0.61 0.199 0.517 0.314 0.726 0.538 0.215 0.175 0.373 0.121 0.099 0.935 0.5 0.591 TNLG v2 (6.7B) 0.242 0.698 0.631 0.21 0.561 0.345 0.704 0.478 0.167 0.158 0.332 0.146 0.11 0.927 0.532 0.525 J1-Large v1 (7.5B) 0.241 0.683 0.623 0.19 0.532 0.328 0.7 0.514 0.197 0.147 0.292 0.134 0.102 0.956 0.532 0.545 GPT-J (6B) 0.249 0.649 0.545 0.156 0.559 0.33 0.663 0.514 0.199 0.152 0.345 0.131 0.096 0.939 0.52 0.619 curie (6.7B) 0.243 0.656 0.604 0.199 0.552 0.321 0.682 0.502 0.232 0.162 0.3 0.113 0.091 0.889 0.539 0.49 Cohere medium v20220720 (6.1B) 0.279 0.659 0.559 0.177 0.504 0.279 0.706 0.496 0.19 0.152 0.374 0.077 0.087 0.935 0.504 0.52 text-babbage-001 0.229 0.451 0.429 0.07 0.33 0.284 0.561 0.452 0.233 0.208 0.449 0.151 0.046 0.913 0.499 0.509 T0pp (11B) 0.407 0 0.151 0.039 0.19 0.121 - - 0.377 - - 0.122 0.09 0.207 0.234 0.118 UL2 (20B) 0.291 0.746 0.083 0.204 0.349 0.144 - - 0.193 - - 0.03 0.058 0.337 0.521 0.404 T5 (11B) 0.29 0.761 0.086 0.194 0.477 0.116 - - 0.133 - - 0.043 0.015 0.379 0.509 0.37 Cohere small v20220720 (410M) 0.264 0.457 0.294 0.078 0.309 0.219 0.483 0.348 0.217 - 0.304 0.063 0.033 0.578 0.501 0.492 ada (350M) 0.243 0.581 0.326 0.082 0.365 0.242 0.435 0.38 0.215 0.102 0.29 0.09 0.022 0.849 0.517 0.423 babbage (1.3B) 0.235 0.574 0.491 0.119 0.451 0.273 0.555 0.438 0.188 0.122 0.317 0.079 0.045 0.597 0.519 0.455 text-ada-001 0.238 0.464 0.238 0.025 0.149 0.176 0.429 0.346 0.232 0.134 0.302 0.136 0.034 0.822 0.503 0.406 YaLM (100B) 0.243 0.634 0.252 0.068 0.227 0.162 - - 0.202 - - 0.017","title":"Result"},{"location":"#future-work","text":"Develop methods to mitigate the potential safety risks associated with using large-scale language models like ChatGPT, such as automatic red teaming, auditing, and adaptive testing. Publish the benchmark dataset and code on a public repository such as GitHub to facilitate future research in the field. Write a research paper detailing the methodology, results, and insights gained from the benchmark study and share it with the scientific community to encourage further research and collaboration.","title":"Future Work"},{"location":"#acknowledgments","text":"","title":"Acknowledgments"}]}